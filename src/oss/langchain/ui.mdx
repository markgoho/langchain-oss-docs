---
title: Agent Chat UI
---

import AlphaCallout from '/snippets/alpha-lc-callout.mdx';

<AlphaCallout />

LangChain provides a powerful prebuilt user interface that work seamlessly with agents created using [`create_agent()`](/oss/langchain/agents). This UI is designed to provide rich, interactive experiences for your agents with minimal setup, whether you're running locally or in a deployed context (such as [LangGraph Platform](https://www.langchain.com/langgraph-platform)).

## Agent Chat UI

Agent Chat UI is a Next.js application that provides a conversational interface for interacting with any LangChain agent. It supports real-time chat, tool visualization, and advanced features like time-travel debugging and state forking.

Agent Chat UI is open source and can be adapted to your application needs. Find the repository [here](https://github.com/langchain-ai/agent-chat-ui).

<Frame>
<iframe
  className="w-full aspect-video rounded-xl"
  src="https://www.youtube.com/embed/lInrwVnZ83o?si=Uw66mPtCERJm0EjU"
  title="Agent Chat UI"
  frameBorder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowFullScreen
/>
</Frame>

### Features

<Accordion title="Tool visualization">
Studio automatically renders tool calls and results in an intuitive interface.
<Frame>
![Tool visualization in Studio](/oss/images/studio_tools.gif)
</Frame>
</Accordion>

<Accordion title="Time-travel debugging">
Navigate through conversation history and fork from any point
<Frame>
![Time-travel debugging in Studio](/oss/images/studio_fork.gif)
</Frame>
</Accordion>

<Accordion title="State inspection">
View and modify agent state at any point during execution
<Frame>
![State inspection in Studio](/oss/images/studio_state.gif)
</Frame>
</Accordion>

<Accordion title="Human-in-the-loop">
Built-in support for reviewing and responding to agent requests
<Frame>
![Human-in-the-Loop in Studio](/oss/images/studio_hitl.gif)
</Frame>
</Accordion>

### Quick start

The fastest way to get started is using the hosted version:

1. **Visit [Agent Chat UI](https://agentchat.vercel.app)**
2. **Connect your agent** by entering your deployment URL or local server address
3. **Start chatting** - the UI will automatically detect and render tool calls and interrupts

### Local development

For customization or local development, you can run Agent Chat UI locally:

<CodeGroup>
```bash Using npx
# Create a new Agent Chat UI project
npx create-agent-chat-app my-chat-ui
cd my-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```
```bash Clone repository
```bash
# Clone the repository
git clone https://github.com/langchain-ai/agent-chat-ui.git
cd agent-chat-ui

# Install dependencies and start
pnpm install
pnpm dev
```
</CodeGroup>

### Connect to your agent

Agent Chat UI can connect to both [local](/oss/langchain/studio#setup-local-langgraph-server) and [deployed agents](/oss/langchain/deploy).

After starting Agent Chat UI, you'll need to configure it to connect to your agent:

1. **Graph ID**: Enter your graph name (find this under `graphs` in your `langgraph.json` file)
2. **Deployment URL**: Your LangGraph server's endpoint (e.g., `http://localhost:2024` for local development, or your deployed agent's URL)
3. **LangSmith API key (optional)**: Add your LangSmith API key (not required if you're using a local LangGraph server)

Once configured, Agent Chat UI will automatically fetch and display any interrupted threads from your agent.

<Tip>
  Agent Chat UI has out-of-the-box support for rendering tool calls and tool result messages. To customize what messages are shown, see [Hiding Messages in the Chat](https://github.com/langchain-ai/agent-chat-ui?tab=readme-ov-file#hiding-messages-in-the-chat).
</Tip>
